###### 2021-03-06
###### 2021-03-09
> - 전이학습 부분을 인터넷을 참고하여 개인적으로 공부함
> - 출처 : https://jeinalog.tistory.com/13

# CNN & 전이학습(Transfer Learning)
> - 밑바닥부터 모델을 만드는 것이 아니라 사전에 학습되어 있는 모델을 불러와 새로운 데이터를 학습시키는 방법
> - 내가 풀고자하는 문제와 비슷한 주제의 데이터와 비슷하면서도 사이즈가 큰 데이터로 미리 학습되어 있는 모델을 불러쓰는 것과 같다.
> - 좋은 성능의 모델을 사용하기 위해서는 많은 데이터로 학습해야 하고 그렇다면 많은 시간이 소요되는데, 이때 전이학습은 이러한 많은 시간을 아낄 수 있는 매우 좋은 방법이다.
> - 컴퓨터 비전에서는 `VGG`와 같은 유명한 모델을 불러서 사용하는 것과 같다.

<br>
<br>
<br>
<br>
<br>


## 1. CNN
> - 이미지 처리에서 사전학습을 많이 사용하기에 CNN으로 전이학습을 설명한다. 그전에 CNN에 대한 구조를 설명한다.
> - 전이학습에 사용되는 몇몇의 사전학습 모델은 큰 사이즈의 합성곱 신경망(CNN)구조를 사용하고 있다.
> - 일반적으로 CNN은 두 가지 파트로 구성되어 있다.
>   - **(1) Convolutional Base**
>       - 합성곱층과 풀링층이 여러겹 쌓여져 있는 부분을 말한다. 
>       - Convolution Base의 주된 목적은 데이터의 특징을 효과적으로 추출하는 것이다.
>   - **(2) Classifier**
>       - 주로 완전 연결 층 (Full Connected Layer)로 이루어져 있다.
>       - 완전 연결 층이란 `모든 계층의 뉴런이 이전 층의 출력노드와 하나도 빠짐없이 모두 연결되어 있는 층`을 뜻한다. 아래 그림을 참고하면 모든 뉴런이 하나도 빠짐없이 연결되어 있음을 볼 수 있다.
>           
>           > ![Full_Connected_Layer](https://user-images.githubusercontent.com/54063179/110203467-cd15c800-7eb1-11eb-88e4-4aa525d67491.png)
>       - Classifier의 목적은 **Convolutional Base에서 추출해낸 특징을 기반으로 학습하여 올바른 클래스 값을 반환하는 것이다.**
>   - 아래 그림은 `(1) Convolutional Base`와 `(2) Classifier`를 가지는 CNN의 구조를 간소화 하여 그린 그림이다.
>       
>       > ![02_CNN](https://user-images.githubusercontent.com/54063179/110203754-3944fb80-7eb3-11eb-808d-5b7c0520b4da.png)


<br>
<br>
<br>
<br>
<br>

## 2. 사전 학습된 모델을 내 프로젝트에 맞게 재정의하기
> - 사전 학습된 모델을 나의 상황에 맞게 사용하기 위해서는 사전모델의 **Classifier**를 없애고 나의 목적에 맞는 **Classifier**로 교체하는 작업부터 시작한다.
> - 그런 다음 Classifier교체로 인해 만들어진 새로운 모델의 **Fine Tune**을 진행한다.
> - 소개하고자 하는 **Fine Tune**방법은 세가지 이다.
>   - **(1) 전략1 : Convolutional Base의 전체와 Classifier를 모두 재학습**
>       - 이 방법은 사전학습 모델의 구조만을 가져와서 모델 내부 노드의 파라매터를 처음부터 다시 학습시키는 방법이다.
>       - 모델의 파라미터가 아닌 구조만을 참고하는 방법이기에 **새로운 모델을 학습시키는 것과 마찬가지로 많은 데이터와 많은 시간이 소요된다.**
>   - **(2) 전략2 : Convolutional Base의 일부분은 고정, 나머지 부분과 Classifier는 새롭게 학습시키기**
>       - 입력층에 가까운 Convolutional Base는 데이터의 일반적인 특징(어떤 데이터에 관계없이 독립적인 특징)을 추출하고 출력층에 가까울 수록 데이터의 특유한 특징(데이터마다 달라지는 특징)을 추출한다.
>       - 이러한 특성을 고려하여 사전학습을 사용하고자 하는 분석가가 임의로 Convolution Base의 어느 층까지 학습을 새로시킬 것인지 정할 수 있다.
>       - 만약 보유하고 있는 데이터셋이 작고, 사전학습된 모델의 크기가 커 파라메터가 많은 상황에서 사전학습 모델의 구조만을 참고하여 재학습한다면 오버피팅이 일어날 수 있기 때문에 Convolution Base의 대부분의 계층을 재학습시키지 않고 그대로 둘 수 있다.
>       - 반면 보유하고 있는 데이터셋이 크고, 사전학습된 모델의 크기가 작아 파라메터가 적은 상황이라면 사전학습 모델의 구조를 참고하여 모델을 재학습 할 때, 오버피팅을 걱장할 우려가 줄어들어 더 많은 Convolution Base의 층을 재학습시켜 자신의 프로젝트에 맞는 모델을 재탄생 시킬 수 있다.
>   - **(3) 전략3 : Convolutional Base의 모든 층을 고정, Classifier만 재학습**
>       - 이 경우는 Convolutional Base를 그대로 두어 특징을 추출하는 메커니즘을 그대로 사용하고 Classifier만 재학습하는 경우를 말한다.
>       - 이 방법은 내가 가지고 있는 컴퓨터의 연산 능력이 부족하거나, 내가 현재 보유하고있는 데이터의 수가 적을 때, 마지막으로 내가 풀고자하는 문제가 사전학습된 모델이 풀고자하는 문제와 매우 유사할때 사용할 수 있는 방법이다. 
>   - 위 세가지 전략(**전략1**, **전략2**, **전략3**) 경우를 그림으로 그리면 다음과 같다.
>       
>       > ![03_transform](https://user-images.githubusercontent.com/54063179/110432027-f4f36e80-80f1-11eb-94c5-50634128f468.png)
>   - **전략3**의 경우는 비교적 간단히 사전모델을 불러와 사용할 수 있지만, **전략1**과 **전략2**의 경우는 불러온 사전모델을 학습시키는 과정이 있기에 **learning rate**에 대하여 조심하여야 한다. 
>   - **learning rate**란? 신경망에서 파라메터들을 얼마나 학습시킬지 결정하는 하이퍼 파라미터이다.
>   - **CNN베이스**의 사전학습 모델을 사용할 때는 이전에 학습한 내용을 모두 잊어버릴 위험성이 있기 때문에 적은 **learning rate**를 사용하는 것이 바람직하다.
>   - **CNN베이스**의 사전학습 모델이 잘 학습되어 있다는 가정하에서 **전략1**또는 **전략2**의 경우에서 적은 **learning rate**를 사용하여 새로운 데이터를 학습시킨다면, 원래학습되어 있던 지식들을 잘 보존하면서 추가로 학습을 해나갈 것이다.


<br>
<br>
<br>
<br>
<br>


## 3. 전이학습의 전체 과정
> 본격적인 실습전에 전이학습의 전체과정을 요약해 본다면 다음과 같다. 
>   - **(1) 사전학습모델 선택하기**
>       - 다양하게 공개되어이있는 사전학습 모델 중에서 내 문제에 적합한 모델을 선택한다.
>       - 만약 `Keras`를 사용한다면, 간단하게 바로 여러가지 사전학습 모델을 불러와 사용할 수 있다. 
>       - 대표적인 사전학습 모델로는 `VGG(Simonyan & Zisserman 2014)`, `Inception(Szegedy et al. 2015)`, `ResNet5(He et al. 2015)` 등이 있다.
>       - https://keras.io/api/applications/ 이 링크를 살펴보면 `Keras`가 제공하는 모든 모델을 확인할 수 있다.
>   - **(2) 내 문제가 `데이터 크기 - 데이터 유사성` 그래프에서 어떤 부분에 속하는지 알아보기**
>       - `데이터의 크기`와 `데이터의 유사성`을 기반으로 네가지 상황으로 구분한다. 다음 그림을 참조하자.
>       > ![그림3](https://user-images.githubusercontent.com/54063179/110436217-4e11d100-80f7-11eb-830e-8f22f57b28a8.png)
>       - 위의 그래프가 제시하는 네가지 상황에서 내가 속한 상황은 무엇인지 알아보고 어떤 전략을 써야하는지 결정할 수 있다.
>       - 여기서 `데이터셋의 크기`는 1,000개 이하의 이미지 데이터셋을 뜻한다.
>       - `데이터셋의 유사성`은 상식의 선에서 생각해 볼 수 있다. 예를들어, 나의 문제가 고양이와 강아지를 분류해야할 상황이면, ImageNet의 모델은 이미 고양이와 강아지에 대해 학습한 상태이기에 내 문제와 유사한 상황이지만, 내 문제가 암세포를 구문해야하는 문제라면 ImageNet은 유사한 데이터셋을 학습한 모델이라 할 수 없다.
>   - **(3) 내 모델을 Fine-Tuning 하기**
>       - `데이터의 크기`와 `데이터의 유사성`을 고려하여 내가 어떠한 상황에 속해있는지 알았다면, 이제 각 상황에 따라 어떻게 Fine-tuning을 진행해야 하는지 결정하고 시행해야 한다. 
>       - 밑의 그림은 `데이터셋의 유사성`과 `데이터셋의 크기`를 고려하였을 때, 어떻게 사전모델의 파라메터를 학습해야하는지(Convolutional Base와 Classifier를 어떻게 학습해야 하는지)를 **전략1**, **전략2**, **전략3**으로 정리한 그림이다.
>       > ![그림4](https://user-images.githubusercontent.com/54063179/110437349-a1d0ea00-80f8-11eb-8903-d5edcd6c89ff.png)
>   - 각각의 네가지 경우에 대해서 **전력1**, **전략2**, **전략3** 중 어떠한 전략이 적절한지 설명한다.
>       > ![그림5](https://user-images.githubusercontent.com/54063179/110437924-3c312d80-80f9-11eb-805a-d81e62a0b3fe.png)
>       > - **Quadrant 1** : 내가 보유한 데이터가 크기가 크고 사전학습 모델과 유사성이 작을 때
>       >   - 이 경우는 **전략1**이 적합하다. 내가 가진 데이터의 크기가 크기 때문에 모델을 처음부터 내가 원하는 목적을 가지도록 학습시킬 수 있다.
>       >   - 비록 사전학습 모델이 내가가진 데이터와 유사성이 적은 데이터로 학습되어 있을지라도, 사전학습 모델의 구조와 하이퍼 파라미터를 참조하는 것은 아예 처음부터 시작하는 것 보다 매우 유용할 것이다.
>       > - **Quadrant 2** : 내가 보유한 데이터의 크기가 크고 사전학습 모델과 유사성도 높을 때
>       >   - 어떠한 전략을 사용해도 상관없지만, 추천하는 전략은 **전략2**이다. 
>       >   - 데이터셋의 크기가 크기 때문에, 오버피팅은 큰 문제가 되지않는다. 그러므로 우리가 원하는 만큼 학습시켜도 된다. 
>       >   - 또한, 내가 보유한 데이터와 사전학습 모델의 유사성도 크기 때문에, 사전학습 모델의 지식(모델의 성능)을 사용하지 않을 이유가 없다. 그러므로 데이터의 특징을 추출하는 층인 Convolution Base 전체를 재학습시킬 필요없다. 그러므로 데이터 마다의 일반적인 특징을 추출하는 Convolution Base의 윗단 정도만 재학습시키고 데이터마다 가지는 고유한 특징을 추출하는 Convolution Base의 밑단만(일부만) 재학습하는 갓으로도 충분할 것이다.
>       > - **Quadrant 3** : 내가 보유한 데이터의 크기도 작고 사전학습 모델과의 유사성도 적을 때
>       >   - 최악의 상황이다. 내가 보유한 데이터를 더욱 수집하거나, 유사성이 높은 데이터를 수집하는 것이 불가능하다면, 적용해볼만한 전략은 **전략2**이다.
>       >   - 하지만, Convolutional Base의 계층 중 몇개의 계층을 그대로 두고 몇개의 계층을 재학습시켜야 할지 알아내는 것은 매우 어렵다. 
>       >   - Convolutional Base의 너무 많은 계층을 재학습시키면 내가 가지고 있는 데이터셋의 크기가 매우 작아 모델이 과적합을 일으킬 수 있다. 
>       >   - 반면 Convolutional Base의 너무 적은 계층만 재학습시킨다면 내가 가지고 있는 데이터셋과 유사성이 매우 적어 모델의 올바른 성능을 기대할 수 없다.
>       >   - 아마 `Quadrant 2`의 상황보다는 Convolutional Base를 좀 더 깊은 층까지 재학습시켜야 할 것이다. 
>       >   - 또한, 작은 데이터셋을 극복하기 위해 `data augmentation`(보유하고 있는 데이터를 왜곡 또는 변혀시켜 데이터의 크기를 증폭시키는 기법)을 적용하는 것 또한 방법이다. 
>       >   - 다음 링크는 `data augmentation`에 대하여 설명되어 있는 홈페이지이다. https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced
>       > - **Quadrant 4** : 내가 보유한 데이터의 크기는 작지만, 사전학습 모델과 유사성이 클 때
>       >   - 이러한 상황에서는 **전략3**이 적절하다.
>       >   - 이 경우 사전학습모델의 `Classifier`만 제거하고 사전학습 모델의 `Convolutional Base`에 새로운 `Classifier`를 장착하여 분류할수 있도록 학습시키면 된다. 
>       >   - 즉, 새로운 `Classifier`만 달아주면 되는 것이다.


<br>
<br>
<br>
<br>
<br>


## 4. Clssifier (분류기)
> - 다시한번 설명하면, CNN신경망은 두가지 부분으로 나뉘어 진다.
>   - `Convolution Base` : 이미지로부터 특징을 추출하는 부분
>   - `Classifier` : 추출된 특징을 입력받아서 최종벅으로 이미지의 카테고리를 결정하는 부분
> - 여기서는 `Classifier` 에 대하여 알아보자
>   - **(1) Fully-connected layers**
>     - 분류 문제에서 표준으로 쓰이는 방법으로, 하나의 **완전 연결 계층(Fully-connected layers)**을 쌓은 후 마지막에 **소프트맥스 활성화 함수 계층(Softmax Activated layer)** 을 놓는 것이다.
>   - **(2) Global average pooling**
>     - 조금 다른 관점으로는 평균 풀링 방법이 있다.
>     - 이 방법은 `Lin et al. 2013` 논문에서 제안된 방법으로 Convolutional Base의 끝에서 바로 전에 설명한 Fully-connected layer 대신에 **평균 풀링 계층**  을 추가하고 그 결과값을 바로 **소프트맥스 계층**과 연결하는 방법이다.
>     - 좀 더 자세히 알아보고 싶으면 `Lin et al.2013` 논문을 참조라하.
>   - **(3) Linear support vector machines**
>     - 선형 서포트 벡터머신은 또 다른 아이디어이다. `Tang(2013)` 의 논문에 따르면, Convolutional Base에서 추출된 특징을 선형 SVM 분류기로 분류함으로써 정확도를 더 높일 수 있다고 한다.