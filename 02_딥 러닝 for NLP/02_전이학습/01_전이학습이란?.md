###### 2021-03-06
> - 전이학습 부분을 인터넷을 참고하여 개인적으로 공부함
> - 출처 : https://jeinalog.tistory.com/13

# 전이학습(Transfer Learning)
> - 밑바닥부터 모델을 만드는 것이 아니라 사전에 학습되어 있는 모델을 불러와 새로운 데이터를 학습시키는 방법
> - 내가 풀고자하는 문제와 비슷한 주제의 데이터와 비슷하면서도 사이즈가 큰 데이터로 미리 학습되어 있는 모델을 불러쓰는 것과 같다.
> - 좋은 성능의 모델을 사용하기 위해서는 많은 데이터로 학습해야 하고 그렇다면 많은 시간이 소요되는데, 이때 전이학습은 이러한 많은 시간을 아낄 수 있는 매우 좋은 방법이다.
> - 컴퓨터 비전에서는 `VGG`와 같은 유명한 모델을 불러서 사용하는 것과 같다.

#
#
#
#
#

## 1. CNN
> - 이미지 처리에서 사전학습을 많이 사용하기에 CNN으로 전이학습을 설명한다. 그전에 CNN에 대한 구조를 설명한다.
> - 전이학습에 사용되는 몇몇의 사전학습 모델은 큰 사이즈의 합성곱 신경망(CNN)구조를 사용하고 있다.
> - 일반적으로 CNN은 두 가지 파트로 구성되어 있다.
>   - **(1) Convolution Base**
>       - 합성곱층과 풀링층이 여러겹 쌓여져 있는 부분을 말한다. 
>       - Convolution Base의 주된 목적은 데이터의 특징을 효과적으로 추출하는 것이다.
>   - **(2) Classifier**
>       - 주로 완전 연결 층 (Full Connected Layer)로 이루어져 있다.
>       - 완전 연결 층이란 `모든 계층의 뉴런이 이전 층의 출력노드와 하나도 빠짐없이 모두 연결되어 있는 층`을 뜻한다. 아래 그림을 참고하면 모든 뉴런이 하나도 빠짐없이 연결되어 있음을 볼 수 있다.
>           > ![Full_Connected_Layer](https://user-images.githubusercontent.com/54063179/110203467-cd15c800-7eb1-11eb-88e4-4aa525d67491.png)
>       - Classifier의 목적은 **Convolution Base에서 추출해낸 특징을 기반으로 학습하여 올바른 클래스 값을 반환하는 것이다.**
>   - 아래 그림은 `(1) Convolution Base`와 `(2) Classifier`를 가지는 CNN의 구조를 간소화 하여 그린 그림이다.
>       > ![02_CNN](https://user-images.githubusercontent.com/54063179/110203754-3944fb80-7eb3-11eb-808d-5b7c0520b4da.png)


#
#
#
#
#


## 2. 사전 학습된 모델을 내 프로젝트에 맞게 재정의하기
> - 사전 학습된 모델을 나의 상황에 맞게 사용하기 위해서는 사전모델의 **Classifier**를 없애고 나의 목적에 맞는 **Classifier**로 교체하는 작업부터 시작한다.
> - 그런 다음 Classifier교체로 인해 만들어진 새로운 모델의 **Fine Tune**을 진행한다.
> - 소개하고자 하는 **Fine Tune**방법은 세가지 이다.
>   - **(1) 전체모델 새로 학습시키기**
>       - 이 방법은 사전학습 모델의 구조만을 가져와서 모델 내부 노드의 파라매터를 처음부터 다시 학습시키는 방법이다.
>       - 모델의 파라미터가 아닌 구조만을 참고하는 방법이기에 **새로운 모델을 학습시키는 것과 마찬가지로 많은 데이터와 많은 시간이 소요된다.**
>   - **(2) Convolution Base의 일부분은 고정, 나머지 부분과 Classifier는 새롭게 학습시키기**
>       - 입력층에 가까운 Convolution Base는 데이터의 일반적인 특징(어떤 데이터에 관계없이 독립적인 특징)을 추출하고 출력층에 가까울 수록 데이터의 특유한 특징(데이터마다 달라지는 특징)을 추출한다.
>       - 이러한 특성을 고려하여 사전학습을 사용하고자 하는 분석가가 임의로 Convolution Base의 어느 층까지 학습을 새로시킬 것인지 정할 수 있다.
>       - 만약 보유하고 있는 데이터셋이 작고, 사전학습된 모델의 크기가 커 파라메터가 많다면, 오버피팅이 일어날 수 있기 때문에 Convolution Base의 대부분의 계층을 재학습시키지 않고 그대로 둘 수 있다.
>       - 반면 보유하고 있는 데이터셋이 크고, 사전학습된 모델의 크기가 작아 파라메터가 적다면, 오버피팅을 걱장할 우려가 줄어들어 더 많은 Convolution Base의 층을 재학습시켜 자신의 프로젝트에 맞는 모델을 재탄생 시킬 수 있다.
>   - **(3) Convolution Base의 모든 층을 고정, Classifier만 재학습**